import streamlit as st
import pickle
import torch
import logging
import os
from huggingface_hub import login
from transformers import pipeline
from llama_index.embeddings.langchain import LangchainEmbedding
from langchain_community.embeddings import HuggingFaceEmbeddings
from llama_index.core import VectorStoreIndex
from dotenv import load_dotenv

# Set up logging
logging.basicConfig(level=logging.INFO)

# Load environment variables from .env file
load_dotenv()

# Get the Hugging Face token from the environment variable
HF_TOKEN = os.getenv("HF_TOKEN")

# Check if the token is correctly loaded
if HF_TOKEN is None:
    logging.error("‚ùå Hugging Face token is not set. Please add it to the .env file.")
else:
    logging.info("‚úÖ Hugging Face token loaded successfully.")

# Ensure Streamlit caches models correctly
@st.cache_resource
def load_llm_and_embed_model():
    try:
        from transformers import pipeline
        model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        # ‚úÖ Initialize Hugging Face text-generation pipeline
        llm = pipeline(
            "text-generation", 
            model=model_name, 
            tokenizer=model_name,  # ‚úÖ Explicitly specify tokenizer
            device=0 if torch.cuda.is_available() else -1 
        )
    
    # ‚úÖ Load Hugging Face Embeddings (Fast and Efficient)
        embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2"))
        logging.info("‚úÖ Models loaded successfully!")

        return llm, embed_model

    except Exception as e:
        logging.error(f"‚ùå Error loading models: {e}")
        raise

# ‚úÖ Load embeddings from saved file
def load_embeddings(file_path="/content/dataset/article_embeddings.pkl"):
    try:
        with open(file_path, 'rb') as f:
            index = pickle.load(f)
        return index
    except Exception as e:
        logging.error(f"‚ùå Error loading embeddings: {e}")
        raise

# ‚úÖ Configure the query engine
def configure_query_engine(index, embed_model, llm):
    try:
        query_engine = index.as_query_engine(llm=llm)
        return query_engine
    except Exception as e:
        logging.error(f"‚ùå Error configuring query engine: {e}")
        raise

# ‚úÖ Main Streamlit Function
def main():
    st.title("üìñ Article Recommendation Search Engine - The Guardian")

    # Add a description for users
    st.markdown("Welcome to the **Article Search Engine**. Ask questions about the articles, and get relevant responses.")

    try:
        # ‚úÖ Load LLM and Embeddings
        llm, embed_model = load_llm_and_embed_model()
        index = load_embeddings()
        query_engine = configure_query_engine(index, embed_model, llm)

        # ‚úÖ User Query Input
        query = st.text_input("üîç Ask a question about articles:")

        if query:
            logging.info(f"Query received: {query}")
            response = query_engine.query(query)
            st.write("üìù Response:", response.response.strip())

            # ‚úÖ Free GPU Memory After Query
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logging.info("‚úÖ CUDA cache cleared.")

    except torch.cuda.OutOfMemoryError as e:
        st.error(f"‚ö†Ô∏è CUDA out of memory error: {str(e)}")
        logging.error(f"CUDA OOM error: {str(e)}")

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logging.warning("‚úÖ CUDA memory cleared, try again.")

    except Exception as e:
        st.error(f"‚ùå An error occurred: {e}")
        logging.error(f"Error: {e}")

if __name__ == "__main__":
    main()
